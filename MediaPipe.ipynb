{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1726859483.311383 1159113 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1726859483.727406 1159957 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1726859483.787244 1159957 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Initialize MediaPipe Pose solution\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "# Extract pose landmarks using MediaPipe\n",
    "def extract_pose_landmarks(image):\n",
    "    results = pose.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    if results.pose_landmarks:\n",
    "        # Get all landmarks in an array\n",
    "        landmarks = [[landmark.x, landmark.y, landmark.z] for landmark in results.pose_landmarks.landmark]\n",
    "        return np.array(landmarks)\n",
    "    return None\n",
    "\n",
    "# Compare landmarks to predefined criteria for evaluating stance\n",
    "def evaluate_stance(landmarks):\n",
    "    if landmarks is None or len(landmarks) == 0:\n",
    "        return \"No Landmarks\", None\n",
    "    \n",
    "    shoulder = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value]\n",
    "    elbow = landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value]\n",
    "    wrist = landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value]\n",
    "    \n",
    "    # Calculate the angle using vector math\n",
    "    angle = np.arccos(np.dot(shoulder - elbow, wrist - elbow) /\n",
    "                      (np.linalg.norm(shoulder - elbow) * np.linalg.norm(wrist - elbow)))\n",
    "    angle_degrees = np.degrees(angle)\n",
    "    \n",
    "    # Define your stance criteria\n",
    "    if 60 <= angle_degrees <= 160:  # Good stance range\n",
    "        return \"Good Stance\", angle_degrees\n",
    "    return \"Bad Stance\", angle_degrees\n",
    "\n",
    "# Evaluate the image for stance\n",
    "def evaluate_image_for_stance(image_path):\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Error: The file '{image_path}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to read the image file '{image_path}'. The file may be corrupt or in an unsupported format.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        image = cv2.resize(image, (640, 480))  # Resize image to 640x480 for consistency\n",
    "    except cv2.error as e:\n",
    "        print(f\"Error resizing the image: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    landmarks = extract_pose_landmarks(image)\n",
    "    if landmarks is not None:\n",
    "        result, angle = evaluate_stance(landmarks)\n",
    "        print(f\"Result: {result}, Elbow Angle: {angle:.2f} degrees\")\n",
    "    else:\n",
    "        print(\"Failed to detect landmarks in the image.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: Good Stance, Elbow Angle: 159.16 degrees\n"
     ]
    }
   ],
   "source": [
    "image_path = \"Cricket Batsman Stance/test/images/images--92-_jpg.rf.3bca276f640642e9e394ac22ab14c0d0.jpg\"  # Replace with the actual image path\n",
    "evaluate_image_for_stance(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1726859469.991203 1159113 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1726859470.462061 1159789 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1726859470.525820 1159793 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "# Extract pose landmarks using MediaPipe\n",
    "def extract_pose_landmarks(frame):\n",
    "    results = pose.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    if results.pose_landmarks:\n",
    "        # Get all landmarks in an array\n",
    "        landmarks = [[landmark.x, landmark.y, landmark.z] for landmark in results.pose_landmarks.landmark]\n",
    "        return np.array(landmarks)\n",
    "    return None\n",
    "\n",
    "# Analyze frames and extract pose landmarks\n",
    "def analyze_video_for_stance(video_path, num_frames=5, img_size=(640, 480)):\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "\n",
    "    for i in frame_indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frame = cv2.resize(frame, img_size)\n",
    "            landmarks = extract_pose_landmarks(frame)\n",
    "            if landmarks is not None:\n",
    "                frames.append(landmarks)\n",
    "\n",
    "    cap.release()\n",
    "    return np.array(frames) if frames else None\n",
    "\n",
    "# Compare landmarks to predefined criteria\n",
    "def evaluate_stance(landmarks):\n",
    "    if landmarks is None or len(landmarks) == 0:\n",
    "        return \"No Landmarks\", None\n",
    "\n",
    "    shoulder = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value]\n",
    "    elbow = landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value]\n",
    "    wrist = landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value]\n",
    "\n",
    "    # Calculate the angle using vector math\n",
    "    angle = np.arccos(np.dot(shoulder - elbow, wrist - elbow) /\n",
    "                      (np.linalg.norm(shoulder - elbow) * np.linalg.norm(wrist - elbow)))\n",
    "    angle_degrees = np.degrees(angle)\n",
    "\n",
    "    # Define your stance criteria\n",
    "    if 70 <= angle_degrees <= 110:  # Good stance range\n",
    "        return \"Good Stance\", angle_degrees\n",
    "    return \"Bad Stance\", angle_degrees\n",
    "\n",
    "# Evaluate the video for stance\n",
    "def evaluate_video_for_stance(video_path):\n",
    "    landmarks = analyze_video_for_stance(video_path)\n",
    "    if landmarks is not None:\n",
    "        results = []\n",
    "        for frame_landmarks in landmarks:\n",
    "            result, angle = evaluate_stance(frame_landmarks)\n",
    "            if result != \"No Landmarks\":\n",
    "                results.append((result, angle))\n",
    "\n",
    "        # Aggregating results\n",
    "        stance_counts = {'Good Stance': 0, 'Bad Stance': 0}\n",
    "        for result, _ in results:\n",
    "            stance_counts[result] += 1\n",
    "\n",
    "        # Determine the final result\n",
    "        if stance_counts['Good Stance'] > stance_counts['Bad Stance']:\n",
    "            print(\"Final Result: Good Stance\")\n",
    "        else:\n",
    "            print(\"Final Result: Bad Stance\")\n",
    "    else:\n",
    "        print(\"Failed to analyze stance.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FRAMES = 30  # Number of frames to analyze\n",
    "IMG_SIZE = (640, 480)  # Resize frames to this size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Result: Bad Stance\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    video_path = \"archive-2/Cover Drive/Cover Drive00012519.mov\"  # Replace with your video path\n",
    "    evaluate_video_for_stance(video_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
