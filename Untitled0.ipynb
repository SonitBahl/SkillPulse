{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AFmiqu4FZl7s"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7TTyGhibhBJa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_0.35_128_no_top.h5\n",
            "\u001b[1m2019640/2019640\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1us/step\n"
          ]
        }
      ],
      "source": [
        "base_model = MobileNetV2(input_shape=(128, 128, 3), include_top=False, weights='imagenet', alpha=0.35)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mxLsJNApZnRj"
      },
      "outputs": [],
      "source": [
        "# Define constants\n",
        "IMG_SIZE = (128, 128)  # Further reduced image size\n",
        "BATCH_SIZE = 2  # Smaller batch size\n",
        "EPOCHS = 60\n",
        "VALIDATION_SPLIT = 0.2\n",
        "NUM_FRAMES = 2  # Fixed number of frames per video\n",
        "\n",
        "# Specify your dataset directory here\n",
        "DATASET_DIR = 'archive-2'\n",
        "\n",
        "# Specify the path where you saved the downloaded weights\n",
        "MOBILENET_WEIGHTS_PATH = '/content/drive/MyDrive/Colab Notebooks/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eOkmHAltZxj6"
      },
      "outputs": [],
      "source": [
        "def extract_frames(video_path, num_frames=NUM_FRAMES):\n",
        "    frames = []\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    if total_frames == 0:\n",
        "        print(f\"Warning: Unable to read frames from {video_path}\")\n",
        "        return None\n",
        "\n",
        "    indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
        "\n",
        "    for i in indices:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "        ret, frame = cap.read()\n",
        "        if ret:\n",
        "            frame = cv2.resize(frame, IMG_SIZE)\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frames.append(frame)\n",
        "        else:\n",
        "            print(f\"Warning: Unable to read frame {i} from {video_path}\")\n",
        "            return None\n",
        "\n",
        "    cap.release()\n",
        "    return np.array(frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9AMcMNvIZzOv"
      },
      "outputs": [],
      "source": [
        "def create_model(num_classes):\n",
        "    try:\n",
        "        # Ensure a valid alpha value\n",
        "        base_model = MobileNetV2(input_shape=(128, 128, 3), include_top=False, weights='imagenet', alpha=0.35)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading MobileNetV2: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=(NUM_FRAMES, 128, 128, 3)),\n",
        "        layers.TimeDistributed(base_model),\n",
        "        layers.GlobalAveragePooling3D(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7WJJg4bIZ05c"
      },
      "outputs": [],
      "source": [
        "def load_dataset(dataset_dir):\n",
        "    video_paths = []\n",
        "    labels = []\n",
        "    for shot_type in os.listdir(dataset_dir):\n",
        "        shot_dir = os.path.join(dataset_dir, shot_type)\n",
        "        if os.path.isdir(shot_dir):\n",
        "            for video_file in os.listdir(shot_dir):\n",
        "                if video_file.endswith('.mov'):\n",
        "                    video_path = os.path.join(shot_dir, video_file)\n",
        "                    frames = extract_frames(video_path)\n",
        "                    if frames is not None:\n",
        "                        video_paths.append(video_path)\n",
        "                        labels.append(shot_type)\n",
        "                    else:\n",
        "                        print(f\"Skipping {video_path} due to frame extraction issues\")\n",
        "    return video_paths, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8T1b8p5NZ24n"
      },
      "outputs": [],
      "source": [
        "def data_generator(video_paths, labels, shot_types, batch_size=BATCH_SIZE):\n",
        "    num_samples = len(video_paths)\n",
        "    while True:\n",
        "        shuffled_indices = np.random.permutation(num_samples)\n",
        "\n",
        "        for i in range(0, num_samples, batch_size):\n",
        "            batch_indices = shuffled_indices[i:i + batch_size]\n",
        "            batch_videos = [video_paths[j] for j in batch_indices]\n",
        "            batch_labels = [labels[j] for j in batch_indices]\n",
        "\n",
        "            batch_frames = np.array([extract_frames(video) for video in batch_videos])\n",
        "\n",
        "            batch_labels_encoded = tf.keras.utils.to_categorical(\n",
        "                [shot_types.index(label) for label in batch_labels],\n",
        "                num_classes=len(shot_types)\n",
        "            )\n",
        "\n",
        "            yield batch_frames, batch_labels_encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bsnIWYVEZ4Sd"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_paths, train_labels, val_paths, val_labels, shot_types):\n",
        "    train_gen = data_generator(train_paths, train_labels, shot_types)\n",
        "    val_gen = data_generator(val_paths, val_labels, shot_types)\n",
        "\n",
        "    steps_per_epoch = len(train_paths) // BATCH_SIZE\n",
        "    validation_steps = len(val_paths) // BATCH_SIZE\n",
        "\n",
        "    model.fit(train_gen,\n",
        "              validation_data=val_gen,\n",
        "              steps_per_epoch=steps_per_epoch,\n",
        "              validation_steps=validation_steps,\n",
        "              epochs=EPOCHS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pB2rHWB3Z7PL"
      },
      "outputs": [],
      "source": [
        "def predict_shot(model, video_path, shot_types):\n",
        "    frames = extract_frames(video_path)\n",
        "    if frames is None:\n",
        "        return \"Error\", 0.0\n",
        "    frames = np.expand_dims(frames, axis=0)\n",
        "    prediction = model.predict(frames)\n",
        "    predicted_shot = shot_types[np.argmax(prediction)]\n",
        "    confidence = np.max(prediction)\n",
        "    return predicted_shot, confidence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBElxbAZZ-GA",
        "outputId": "3bac55c1-c52f-4042-9431-2ae557980401"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detected shot types: ['Cover Drive', 'Pull Shot', 'Ramp Shot', 'Reverse Sweep', 'Straight Drive', 'Upper Cut']\n",
            "Epoch 1/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 110ms/step - accuracy: 0.2311 - loss: 1.9149 - val_accuracy: 0.1522 - val_loss: 2.1155\n",
            "Epoch 2/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 97ms/step - accuracy: 0.2854 - loss: 1.6475 - val_accuracy: 0.2000 - val_loss: 2.1774\n",
            "Epoch 3/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 104ms/step - accuracy: 0.3767 - loss: 1.5369 - val_accuracy: 0.1333 - val_loss: 2.0420\n",
            "Epoch 4/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 98ms/step - accuracy: 0.3583 - loss: 1.5626 - val_accuracy: 0.2444 - val_loss: 1.9080\n",
            "Epoch 5/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 104ms/step - accuracy: 0.3658 - loss: 1.4963 - val_accuracy: 0.2222 - val_loss: 1.8312\n",
            "Epoch 6/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 105ms/step - accuracy: 0.5131 - loss: 1.3489 - val_accuracy: 0.4667 - val_loss: 1.3513\n",
            "Epoch 7/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 101ms/step - accuracy: 0.5005 - loss: 1.2857 - val_accuracy: 0.5111 - val_loss: 1.2513\n",
            "Epoch 8/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 96ms/step - accuracy: 0.5090 - loss: 1.2848 - val_accuracy: 0.5556 - val_loss: 1.1134\n",
            "Epoch 9/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 96ms/step - accuracy: 0.5484 - loss: 1.0967 - val_accuracy: 0.5778 - val_loss: 1.1892\n",
            "Epoch 10/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 94ms/step - accuracy: 0.5271 - loss: 1.2834 - val_accuracy: 0.4889 - val_loss: 1.0783\n",
            "Epoch 11/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 96ms/step - accuracy: 0.5826 - loss: 1.1526 - val_accuracy: 0.5111 - val_loss: 1.2095\n",
            "Epoch 12/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 97ms/step - accuracy: 0.5673 - loss: 1.1144 - val_accuracy: 0.6222 - val_loss: 0.9768\n",
            "Epoch 13/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 99ms/step - accuracy: 0.6978 - loss: 0.9439 - val_accuracy: 0.6889 - val_loss: 0.9325\n",
            "Epoch 14/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 99ms/step - accuracy: 0.6223 - loss: 1.0239 - val_accuracy: 0.6444 - val_loss: 0.9587\n",
            "Epoch 15/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 100ms/step - accuracy: 0.6961 - loss: 0.7941 - val_accuracy: 0.6667 - val_loss: 0.8554\n",
            "Epoch 16/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 98ms/step - accuracy: 0.7178 - loss: 0.8335 - val_accuracy: 0.5778 - val_loss: 0.9519\n",
            "Epoch 17/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 97ms/step - accuracy: 0.7338 - loss: 0.8917 - val_accuracy: 0.7111 - val_loss: 0.7590\n",
            "Epoch 18/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 103ms/step - accuracy: 0.7014 - loss: 0.8418 - val_accuracy: 0.6222 - val_loss: 1.1136\n",
            "Epoch 19/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 98ms/step - accuracy: 0.7407 - loss: 0.7377 - val_accuracy: 0.7111 - val_loss: 0.9399\n",
            "Epoch 20/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 98ms/step - accuracy: 0.7777 - loss: 0.6775 - val_accuracy: 0.6444 - val_loss: 1.0834\n",
            "Epoch 21/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 100ms/step - accuracy: 0.7930 - loss: 0.5915 - val_accuracy: 0.6000 - val_loss: 1.0865\n",
            "Epoch 22/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 96ms/step - accuracy: 0.7757 - loss: 0.6267 - val_accuracy: 0.7111 - val_loss: 0.8280\n",
            "Epoch 23/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 98ms/step - accuracy: 0.8306 - loss: 0.5448 - val_accuracy: 0.7111 - val_loss: 0.9313\n",
            "Epoch 24/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 98ms/step - accuracy: 0.8688 - loss: 0.4777 - val_accuracy: 0.6667 - val_loss: 1.1563\n",
            "Epoch 25/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 100ms/step - accuracy: 0.7980 - loss: 0.6512 - val_accuracy: 0.6957 - val_loss: 1.0685\n",
            "Epoch 26/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 97ms/step - accuracy: 0.7806 - loss: 0.6256 - val_accuracy: 0.6667 - val_loss: 1.0470\n",
            "Epoch 27/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 97ms/step - accuracy: 0.8604 - loss: 0.4671 - val_accuracy: 0.7111 - val_loss: 1.0558\n",
            "Epoch 28/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 99ms/step - accuracy: 0.8185 - loss: 0.4950 - val_accuracy: 0.7111 - val_loss: 0.9054\n",
            "Epoch 29/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 98ms/step - accuracy: 0.9001 - loss: 0.4063 - val_accuracy: 0.6444 - val_loss: 1.0047\n",
            "Epoch 30/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 97ms/step - accuracy: 0.8519 - loss: 0.4566 - val_accuracy: 0.7556 - val_loss: 0.7460\n",
            "Epoch 31/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 106ms/step - accuracy: 0.8821 - loss: 0.3732 - val_accuracy: 0.6889 - val_loss: 0.8447\n",
            "Epoch 32/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 119ms/step - accuracy: 0.8422 - loss: 0.5040 - val_accuracy: 0.5111 - val_loss: 1.7826\n",
            "Epoch 33/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 102ms/step - accuracy: 0.8599 - loss: 0.3904 - val_accuracy: 0.6667 - val_loss: 1.2268\n",
            "Epoch 34/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 100ms/step - accuracy: 0.8975 - loss: 0.3012 - val_accuracy: 0.7778 - val_loss: 1.1020\n",
            "Epoch 35/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 101ms/step - accuracy: 0.9201 - loss: 0.2618 - val_accuracy: 0.7778 - val_loss: 0.9690\n",
            "Epoch 36/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 98ms/step - accuracy: 0.9247 - loss: 0.3264 - val_accuracy: 0.6667 - val_loss: 1.7663\n",
            "Epoch 37/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 97ms/step - accuracy: 0.9206 - loss: 0.3543 - val_accuracy: 0.6444 - val_loss: 1.1828\n",
            "Epoch 38/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 98ms/step - accuracy: 0.9050 - loss: 0.3577 - val_accuracy: 0.6889 - val_loss: 1.3159\n",
            "Epoch 39/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 97ms/step - accuracy: 0.9361 - loss: 0.2729 - val_accuracy: 0.6889 - val_loss: 1.1501\n",
            "Epoch 40/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 98ms/step - accuracy: 0.8757 - loss: 0.3199 - val_accuracy: 0.5778 - val_loss: 1.3864\n",
            "Epoch 41/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 95ms/step - accuracy: 0.9031 - loss: 0.2432 - val_accuracy: 0.7556 - val_loss: 0.6581\n",
            "Epoch 42/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 97ms/step - accuracy: 0.8399 - loss: 0.4245 - val_accuracy: 0.7333 - val_loss: 1.0663\n",
            "Epoch 43/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 96ms/step - accuracy: 0.9326 - loss: 0.2313 - val_accuracy: 0.7333 - val_loss: 0.9402\n",
            "Epoch 44/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 96ms/step - accuracy: 0.9400 - loss: 0.2285 - val_accuracy: 0.5556 - val_loss: 1.4721\n",
            "Epoch 45/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 100ms/step - accuracy: 0.9081 - loss: 0.2151 - val_accuracy: 0.7111 - val_loss: 0.8744\n",
            "Epoch 46/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 96ms/step - accuracy: 0.9172 - loss: 0.3078 - val_accuracy: 0.6889 - val_loss: 1.1041\n",
            "Epoch 47/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 96ms/step - accuracy: 0.8842 - loss: 0.4688 - val_accuracy: 0.6444 - val_loss: 1.3702\n",
            "Epoch 48/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 100ms/step - accuracy: 0.9093 - loss: 0.3288 - val_accuracy: 0.7556 - val_loss: 0.9691\n",
            "Epoch 49/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 95ms/step - accuracy: 0.9438 - loss: 0.2608 - val_accuracy: 0.7174 - val_loss: 1.1029\n",
            "Epoch 50/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 97ms/step - accuracy: 0.9014 - loss: 0.3331 - val_accuracy: 0.7556 - val_loss: 1.0871\n",
            "Epoch 51/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 98ms/step - accuracy: 0.9572 - loss: 0.1859 - val_accuracy: 0.7333 - val_loss: 1.1106\n",
            "Epoch 52/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 99ms/step - accuracy: 0.9658 - loss: 0.1197 - val_accuracy: 0.7556 - val_loss: 1.0156\n",
            "Epoch 53/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 99ms/step - accuracy: 0.9742 - loss: 0.1395 - val_accuracy: 0.7556 - val_loss: 0.9156\n",
            "Epoch 54/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 96ms/step - accuracy: 0.9258 - loss: 0.2667 - val_accuracy: 0.7556 - val_loss: 0.9384\n",
            "Epoch 55/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 97ms/step - accuracy: 0.9408 - loss: 0.1974 - val_accuracy: 0.6444 - val_loss: 1.8417\n",
            "Epoch 56/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 100ms/step - accuracy: 0.9599 - loss: 0.1220 - val_accuracy: 0.6889 - val_loss: 1.5394\n",
            "Epoch 57/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 100ms/step - accuracy: 0.9434 - loss: 0.2325 - val_accuracy: 0.6222 - val_loss: 1.0890\n",
            "Epoch 58/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 99ms/step - accuracy: 0.9722 - loss: 0.1124 - val_accuracy: 0.6000 - val_loss: 1.4411\n",
            "Epoch 59/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 100ms/step - accuracy: 0.9712 - loss: 0.1046 - val_accuracy: 0.5778 - val_loss: 1.6527\n",
            "Epoch 60/60\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 106ms/step - accuracy: 0.9655 - loss: 0.1649 - val_accuracy: 0.6000 - val_loss: 1.8868\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "Predicted shot: Upper Cut\n",
            "Confidence: 0.66\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    video_paths, labels = load_dataset(DATASET_DIR)\n",
        "\n",
        "    shot_types = sorted(list(set(labels)))\n",
        "    print(f\"Detected shot types: {shot_types}\")\n",
        "    NUM_CLASSES = len(shot_types)\n",
        "\n",
        "    train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
        "        video_paths, labels, test_size=VALIDATION_SPLIT, stratify=labels, random_state=42\n",
        "    )\n",
        "\n",
        "    model = create_model(NUM_CLASSES)\n",
        "    if model is not None:\n",
        "        train_model(model, train_paths, train_labels, val_paths, val_labels, shot_types)\n",
        "\n",
        "        model.save('cricket_shot_classifier.h5')\n",
        "\n",
        "        test_video = 'WhatsApp Video 2024-09-20 at 19.21.06.mov'\n",
        "        predicted_shot, confidence = predict_shot(model, test_video, shot_types)\n",
        "        print(f\"Predicted shot: {predicted_shot}\")\n",
        "        print(f\"Confidence: {confidence:.2f}\")\n",
        "    else:\n",
        "        print(\"Model creation failed. Please check the weights file path and try again.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "YOLO MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current working directory: /Users/sarthakagrawal/SkillPulse\n",
            "Contents of Cricket Batsman Stance folder:\n",
            "['README.roboflow.txt', 'valid', 'README.dataset.txt', '.DS_Store', 'test', 'data.yaml', 'train']\n",
            "Dataset structure looks correct.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/sarthakagrawal/SkillPulse/yenv/lib/python3.12/site-packages/ultralytics/nn/tasks.py:567: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(file, map_location='cpu'), file  # load\n",
            "New https://pypi.org/project/ultralytics/8.2.98 available 😃 Update with 'pip install -U ultralytics'\n",
            "Ultralytics YOLOv8.0.196 🚀 Python-3.12.6 torch-2.4.1 CPU (Apple M2)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/Users/sarthakagrawal/SkillPulse/Cricket Batsman Stance/data.yaml, epochs=50, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=/Users/sarthakagrawal/SkillPulse/runs/detect/train13\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
            "Model summary: 225 layers, 3011043 parameters, 3011027 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir /Users/sarthakagrawal/SkillPulse/runs/detect/train13', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "/Users/sarthakagrawal/SkillPulse/yenv/lib/python3.12/site-packages/ultralytics/engine/trainer.py:238: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = amp.GradScaler(enabled=self.amp)\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/sarthakagrawal/SkillPulse/Cricket Batsman Stance/train/labels.cache... 403 images, 1 backgrounds, 0 corrupt: 100%|██████████| 403/403 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/sarthakagrawal/SkillPulse/Cricket Batsman Stance/valid/labels.cache... 14 images, 0 backgrounds, 0 corrupt: 100%|██████████| 14/14 [00:00<?, ?it/s]\n",
            "Plotting labels to /Users/sarthakagrawal/SkillPulse/runs/detect/train13/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1m/Users/sarthakagrawal/SkillPulse/runs/detect/train13\u001b[0m\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       1/50         0G      1.538      2.442      1.699          7        640: 100%|██████████| 26/26 [05:24<00:00, 12.48s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:02<00:00,  2.10s/it]\n",
            "                   all         14         14    0.00333          1      0.431      0.168\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       2/50         0G      1.438      2.067      1.591         15        640: 100%|██████████| 26/26 [05:15<00:00, 12.14s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:02<00:00,  2.08s/it]\n",
            "                   all         14         14      0.709      0.214      0.336      0.159\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       3/50         0G      1.461      1.941      1.602         10        640: 100%|██████████| 26/26 [04:53<00:00, 11.29s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:02<00:00,  2.41s/it]\n",
            "                   all         14         14      0.354     0.0714     0.0625     0.0128\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       4/50         0G      1.486      1.846      1.607          9        640: 100%|██████████| 26/26 [05:03<00:00, 11.68s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.95s/it]\n",
            "                   all         14         14     0.0849      0.571     0.0701     0.0192\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       5/50         0G      1.516      1.908      1.666          7        640: 100%|██████████| 26/26 [05:14<00:00, 12.11s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.68s/it]\n",
            "                   all         14         14      0.292      0.649      0.319      0.159\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       6/50         0G      1.477      1.821      1.621          7        640: 100%|██████████| 26/26 [04:58<00:00, 11.48s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:02<00:00,  2.25s/it]\n",
            "                   all         14         14      0.551      0.714      0.655      0.323\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       7/50         0G      1.519      1.757      1.612          7        640: 100%|██████████| 26/26 [05:15<00:00, 12.15s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.93s/it]\n",
            "                   all         14         14      0.533      0.652      0.647      0.258\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       8/50         0G      1.425      1.605      1.557          7        640: 100%|██████████| 26/26 [04:50<00:00, 11.19s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.97s/it]\n",
            "                   all         14         14      0.402      0.357      0.381      0.203\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       9/50         0G      1.419      1.593      1.569          9        640: 100%|██████████| 26/26 [06:15<00:00, 14.46s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:03<00:00,  3.66s/it]\n",
            "                   all         14         14      0.163      0.429      0.152     0.0556\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      10/50         0G      1.424      1.547      1.548         10        640: 100%|██████████| 26/26 [06:01<00:00, 13.91s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:02<00:00,  2.38s/it]\n",
            "                   all         14         14      0.348      0.786      0.361      0.187\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      11/50         0G      1.428      1.523      1.564          9        640: 100%|██████████| 26/26 [05:04<00:00, 11.72s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:02<00:00,  2.10s/it]\n",
            "                   all         14         14      0.466      0.786      0.505      0.187\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      12/50         0G      1.357      1.481      1.509          6        640: 100%|██████████| 26/26 [05:49<00:00, 13.42s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:02<00:00,  2.21s/it]\n",
            "                   all         14         14       0.39      0.857      0.424      0.173\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      13/50         0G      1.342      1.426      1.508         12        640: 100%|██████████| 26/26 [04:44<00:00, 10.95s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.76s/it]\n",
            "                   all         14         14      0.442      0.792      0.639      0.446\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      14/50         0G      1.335      1.422      1.507          6        640: 100%|██████████| 26/26 [04:54<00:00, 11.31s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.67s/it]\n",
            "                   all         14         14      0.684      0.786      0.806      0.536\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      15/50         0G      1.308      1.328      1.462          7        640: 100%|██████████| 26/26 [05:18<00:00, 12.24s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.92s/it]\n",
            "                   all         14         14      0.395      0.714      0.531      0.301\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      16/50         0G      1.317      1.295      1.443         13        640: 100%|██████████| 26/26 [04:52<00:00, 11.23s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.71s/it]\n",
            "                   all         14         14      0.493      0.905      0.699      0.368\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      17/50         0G      1.293      1.232      1.428          5        640: 100%|██████████| 26/26 [2:50:10<00:00, 392.72s/it]   \n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.97s/it]\n",
            "                   all         14         14      0.547      0.929      0.724      0.288\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      18/50         0G      1.287      1.226      1.452          7        640: 100%|██████████| 26/26 [1:08:42<00:00, 158.54s/it]  \n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:02<00:00,  2.51s/it]\n",
            "                   all         14         14      0.402      0.816      0.666       0.31\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      19/50         0G      1.269      1.238      1.437         10        640: 100%|██████████| 26/26 [05:32<00:00, 12.78s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:02<00:00,  2.40s/it]\n",
            "                   all         14         14      0.842      0.763      0.846      0.508\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      20/50         0G      1.274      1.224       1.44          9        640: 100%|██████████| 26/26 [05:32<00:00, 12.78s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:02<00:00,  2.80s/it]\n",
            "                   all         14         14      0.485      0.929      0.708      0.469\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      21/50         0G      1.252      1.192      1.418          7        640: 100%|██████████| 26/26 [05:29<00:00, 12.69s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:02<00:00,  2.18s/it]\n",
            "                   all         14         14      0.767      0.786      0.791      0.348\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      22/50         0G       1.18      1.114      1.367         11        640: 100%|██████████| 26/26 [05:59<00:00, 13.82s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:02<00:00,  2.84s/it]\n",
            "                   all         14         14      0.635      0.714      0.709      0.387\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      23/50         0G      1.214      1.189      1.398         11        640: 100%|██████████| 26/26 [05:52<00:00, 13.55s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:02<00:00,  2.04s/it]\n",
            "                   all         14         14      0.523      0.857      0.658      0.334\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      24/50         0G      1.191      1.162      1.397          5        640: 100%|██████████| 26/26 [05:40<00:00, 13.09s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.75s/it]\n",
            "                   all         14         14      0.916      0.781      0.893      0.454\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      25/50         0G      1.162      1.137      1.405         10        640: 100%|██████████| 26/26 [06:01<00:00, 13.89s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.96s/it]\n",
            "                   all         14         14      0.846      0.571      0.704      0.497\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      26/50         0G      1.135       1.11      1.356          5        640: 100%|██████████| 26/26 [06:39<00:00, 15.37s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.72s/it]\n",
            "                   all         14         14       0.72          1      0.944      0.508\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      27/50         0G      1.162      1.092       1.38          6        640: 100%|██████████| 26/26 [05:25<00:00, 12.53s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.70s/it]\n",
            "                   all         14         14      0.561          1      0.805       0.35\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      28/50         0G      1.185       1.07      1.371          6        640: 100%|██████████| 26/26 [05:13<00:00, 12.07s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:02<00:00,  2.21s/it]\n",
            "                   all         14         14      0.528      0.929      0.687      0.407\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      29/50         0G      1.132      1.026      1.392         43        640:  15%|█▌        | 4/26 [00:55<05:18, 14.46s/it]"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from ultralytics import YOLO\n",
        "\n",
        "def check_dataset_structure(dataset_path):\n",
        "    if not os.path.exists(dataset_path):\n",
        "        raise FileNotFoundError(f\"Dataset path '{dataset_path}' does not exist.\")\n",
        "    \n",
        "    data_yaml = os.path.join(dataset_path, \"data.yaml\")\n",
        "    if not os.path.exists(data_yaml):\n",
        "        raise FileNotFoundError(f\"data.yaml not found in '{dataset_path}'.\")\n",
        "    \n",
        "    train_images = os.path.join(dataset_path, \"train\", \"images\")\n",
        "    valid_images = os.path.join(dataset_path, \"valid\", \"images\")\n",
        "    \n",
        "    if not os.path.exists(train_images):\n",
        "        raise FileNotFoundError(f\"Tr aining images not found in '{train_images}'.\")\n",
        "    if not os.path.exists(valid_images):\n",
        "        raise FileNotFoundError(f\"Validation images not found in '{valid_images}'.\")\n",
        "\n",
        "    print(\"Dataset structure looks correct.\")\n",
        "\n",
        "# Print current working directory\n",
        "print(\"Current working directory:\", os.getcwd())\n",
        "\n",
        "# Set the dataset path\n",
        "dataset_path = \"/Users/sarthakagrawal/SkillPulse/Cricket Batsman Stance\"\n",
        "\n",
        "# List contents of the directory\n",
        "print(\"Contents of Cricket Batsman Stance folder:\")\n",
        "print(os.listdir(dataset_path))\n",
        "\n",
        "try:\n",
        "    # Check the dataset structure\n",
        "    check_dataset_structure(dataset_path)\n",
        "    \n",
        "    # Load a pre-trained YOLOv8 model\n",
        "    model = YOLO(\"yolov8n.pt\")\n",
        "    \n",
        "    # Train the model on your dataset\n",
        "    model.train(data=os.path.join(dataset_path, \"data.yaml\"), epochs=50, imgsz=640)\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"Please check your dataset structure and paths.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mval()\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "model.val()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Predict on an image\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(source\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCricket Batsman Stance/test/images/download--1-_jpg.rf.04063f4fc3399cd5fac683582e53e259.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m, show\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "# Predict on an image\n",
        "results = model.predict(source=\"Cricket Batsman Stance/test/images/download--1-_jpg.rf.04063f4fc3399cd5fac683582e53e259.jpg\", show=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
